---
title: "Finding Your True Worth"
date: "2026-02-07"
published: true
description: "Your value isn't in typing code—it's in what you know. In a GenAI world, that tacit knowledge is more precious than ever."
tags:
  - GenAI
  - Software Engineering
  - Career
  - Knowledge Work
image: images/finding-your-true-worth-banner.png
imageAlt: "Silver-haired woman in cyberpunk workshop setting, gesturing toward holographic knowledge visualization while teaching others to see their hidden value"
---

I was running workshops this week on building AGENTS.md files—those plain-text instruction files that tell AI coding agents how to navigate and contribute to your codebase. Think of them as onboarding documents for robots: here's where we put things, here's how we name things, here's what matters and why.

In each I asked for volunteers to share what they'd written. And beautiful things happened.

One developer had captured years of institutional knowledge in a few paragraphs. Another had articulated patterns so deeply internalized she'd never spoken them aloud. With each share, I found myself pointing out the same thing: "Do you see what you just did? You took tacit knowledge—stuff that lived only in your head—and made it explicit. You saved that agent from wandering through dozens of files trying to figure out what you already know."

Here's the thing: many of them didn't realize how much they knew until they tried to teach a machine.

## The Booch Interview and the Panic Cycle

I'd been thinking about this all week, since watching [Gergely Orosz's interview with Grady Booch](https://youtu.be/OfMAtaocvJw). Booch has been around long enough to see these panic cycles before. Compilers. High-level languages. Frameworks. Each wave makes some skills less valuable, but expands what gets built and shifts demand upward.

His framing stuck with me: software engineering is mostly decision-making under constraints, not typing code. We balance forces—physics and computation limits, economics, team dynamics, legal boundaries, and especially ethics. Code is just one instrument in that broader practice.

So why do we keep reducing ourselves to the typing?

## What AI Actually Automates

Booch predicts automation will eat the "pipeline glue" work—the repetitive, messy stuff like CI/CD scaffolding, infrastructure-as-code templates, simple CRUD apps. The well-worn patterns. And he's right. LLMs are brilliant at those because the distance between intent and output is short and predictable.

But here's what they can't do: they can't know that your team tried microservices in 2019 and it was a disaster because of the org structure at the time. They can't know that the reason you never use that particular library is because of a security incident that never made it into the public CVE database. They can't know that Janet in accounting has a workflow that depends on that weird edge case in the billing module.

That's not in the code. That's in *you*.

## The Real Risk

The danger isn't that AI will replace developers. The danger is that companies—and developers themselves—will believe the only value developers provide is typing code. And if you believe that, you'll act accordingly. You'll optimize for keystrokes. You'll measure productivity in lines. You'll outsource everything to the machine and wonder why your systems grow brittle and your institutional memory evaporates.

I've watched this happen before, in different forms. Every time we've treated knowledge workers as interchangeable units of output, we've paid for it later. The cost just shows up somewhere else: in bugs that take weeks to diagnose, in architectural decisions that seemed fine until they weren't, in the slow erosion of the judgment that keeps complex systems alive.

## Proving It to Yourself First

It's hard—and sometimes threatening—to think about your own worth. Especially when everyone around you seems to believe that value equals visible output. That if you're not typing, you're not contributing.

But before you fall prey to that narrative, prove it to yourself first.

Write that AGENTS.md file. Or an onboarding document. Or just try to explain, in plain language, how your system actually works and why it works that way. Watch how much you know that you didn't know you knew.

That knowledge didn't come from nowhere. It came from years of decisions, mistakes, conversations, debugging sessions at 2 AM, and quiet observations of how things really work versus how they're supposed to work. It's the sediment of experience.

And right now, it's more valuable than ever—because it's exactly what the machines can't generate on their own.

## The Durable Advantage

Booch talks about the winners in this transition: the ones who move up to architecture, socio-technical coordination, risk management, and designing systems that endure. That's not about being smarter than AI. It's about being *present* in ways AI can't be. Understanding the humans in the system. Holding the context. Making judgment calls that require knowing the full story.

The abstraction level is rising. LLMs are just the next interface layer. But someone still has to know what to build and why. Someone has to hold the vision, navigate the constraints, and make the calls that can't be automated.

That someone might be you. If you believe it.

So go look at your own work, and ask: what do I know that nobody wrote down? What decisions have I made that only made sense because I understood the context? What would be lost if I walked away tomorrow?

That's your true worth. And it's not going anywhere—unless you let someone convince you it doesn't exist.
